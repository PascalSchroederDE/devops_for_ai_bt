

# Discussion {#sec:discussion}

In this chapter, the results described in chapter \ref{sec:result} will be compared and discussed. In doing so, the criteria defined in chapter \ref{sec:catalogue} will serve as a basis to evaluate the scope, usability, flexibility, and simplicity of the different approaches for building a pipeline. After that, the adapted DevOps will be evaluated and the challenges will be discussed. Last an outlook will be given, in which future opportunities with such technologies and potential for improvement will be delighted.

## Pipeline comparison {#sec:pipelineeval}

During this work, two frameworks - Azure Machine Learning service for a simple and quick deployment as well as Kubeflow as a flexible, multi cloud solution  - has been used to build an example pipeline following the principles defined in chapter \ref{sec:devopsai}. Additionally, these frameworks have been evaluated and tested for additional required functionalities for fulfilling the needs of DevOps for Machine Learning described in chapter \ref{sec:devopsai}. The results of these tests will be evaluated in this chapter on the basis of chapter \ref{sec:catalogue}.

For this the DevOps sets and practices should be called in rembrenence. In figure \ref{fig:devops_architectureII} can be seen, that there were four sets - Steer, Develop & Test, Deploy and Operate. Even if the main focus on the created pipelines are on support for the developing and testing sets, it also accompanies the whole product lifecycle.

![DevOps reference architecture[@Sharma2017, 10]](images/chapter2/devops_architecture.png){ width=400px #fig:devops_architectureII}

The **steer** set was about *planning and managing* a project, which is not the task of a pipeline in first place. As mentioned in chapter \ref{sec:devopsai}, for Machine Learning this also includes *understanding of the data*. A good visualization is an essential point of this, which can be provided by both frameworks. Still, Azure ML service offers an easier way to this, because the loaded dataset can be directly visualized by just right clicking on the node and clicking on *Visualize*. To enable such a visualization for Kubeflow an artifact has to be created, which shows all the data in a table. 

The next set is about **development and testing**. There different factors are of importance. First, the factors influencing the development itself will be discussed.

The first factor is the simplicity of the frameworks so that Data Scientists can focus on their main work instead of infrastructural setups and coding. For that, the Azure Machine Learning service offers an easy way to build a working pipeline from scratch quickly. For the most basic steps of AI development, there are preexisting components defined, which can be added to the pipeline by simply dragging and dropping them to the workbench. The in- and outputs can be connected by connecting the input of one component with the output of another. Also, the configurations can be made directly on the visual interface. So there is no need for coding experience at all, and the interface is easy to use.

To deploy Kubeflow locally some knowledge of Kubernetes deployments is necessary to deploy the Kubeflow framework on Kubernetes. This can cause several problems, for example with the DNS resolution, some crashing pods or unavailable ports as the preparation of this project pointed out through the usual complexity of implementations of such frameworks. After the preparations are done or an available Kubeflow deployment can be used, it offers some predefined pipelines, which can be used for similar tasks as its original purpose. If the developer wants to build an individual pipeline, this pipeline needs to be built from scratch. This requires coding skills because the pipelines are written in Python with the Python SDK. If the developer also needs specific components as Docker containers, which are not publicly available, these need to be coded as well. Additionally, Docker skills are required to build the containers of these components. Compared to the Azure Machine Learning service, all this is more complicated and time-consuming. This means, that the learning curve of Azure is very fast at the beginning, but the possibilities are limited. Kubeflow needs more time to get familiar with, but in exchange there are more possibilities and a higher flexibility.

Another factor to be included in the evaluation was the option to adapt it to new frameworks and technologies enveloping over time. In the case of the Azure Machine Learning service, this completely depends on Microsoft's support for these, because the whole environment depends on the Microsoft Azure cloud. This means that the developer does not have to care about its deployment, technologies, security, and efficiency, but can not decide what to use for himself. 

Kubeflow mainly depends on two technologies - Kubernetes and Containerization. Which technologies, ML frameworks or tools to use beyond that is free choosable by the developer, because the components can be created completely independent from Kubeflow, so every upcoming technology can be used without worrying about compatibility.

Also, both approaches offer the opportunity to pass on some parameters to influence the data preparation, building, and training of the model. In case of Kubeflow, these parameters can be defined by the developer, in case of Azure ML service these are predefined, so not everything is configurable.

The visualization of the results is good in both cases. Azure ML pipelines offer a visualization directly on the interface and give every necessary information as can be seen in chapter \ref{sec:azurepip}. The visualization of Kubeflow has to be enabled by adding an artifact to the corresponding stage. The way of visualizing the results is then up to the developer.

Next, the scope of functionality has to be compared. Azure ML pipeline offers some predefined components, which makes it very easy to use. In exchange for its simplicity, there are not enough components to fulfill every possible need. For this Azure ML offers the possibility to insert Python scripts as a component. Still, the usage of those is not as flexible as it has to be to enable the developer to build the pipeline flexibly. Also, the in- and output of the components is very strict, and the model architecture can not be chosen freely. This can lead to some issues, for example, because the Classifier model is not compatible with multiple columns as labels. This eliminates the opportunity to encode the categories with *One-Hot-Encoding* and aggravates the performance of the resulting model.

Kubeflow, on the other hand, offers full flexibility and functionality, because the components can be created in the way the developer chooses. This requires coding skills of the developer but enables a flexible pipeline without any compromise of functionalities compared to local building and training a model. 

Next, possibilities of *collaborative development* and *continuous integration* will be evaluated.

Both frameworks have the possibility of using an IDE to handle the necessary code. In case of Azure ML service, this can be either done on the visual interface itself or via the Visual Studio SDK for Azure ML service. Kubeflow offers the opportunity to directly integrate and collaborate with Jupyter Notebooks running on Jupyter Hub. Alternatively, the components can be coded as containers in whatever IDE the developer prefers.

Another point is the support of data collaboration platforms so that it can be worked together on data as it is already possible with code. In Azure, the datasets are usually stored directly on Microsoft servers and can be uploaded. It is also possible to create nodes, which connects to data services like Hive Query or Azure Blob storage. With some of these tools, versioning and collaborating on the data is possible.

Because of Kubeflow components can be written freely, it is also possible, that it accesses data from every online data collaboration platform, like Quilt or Git LFS. This also enables the possibility to versionize the data.

Also, tools for labeling data, managing its quality and collaborating with a team are emerging. An example is Labelbox, to name just one, which offers simple data labeling and management, collaboration, and even some automation features. This labeled and prepared data then needs to be available from a shared repository, to which the whole team can contribute to so that they can collaborate.

Regarding *Continuous testing*, in both frameworks components can be added, which directly and automatically test the resuling models every time a new model is built. The results can directly be visualized as mentioned above. All this offers quite a good opportunity to continuosly testing the models.

The third set was all about **delivery**, which includes deploying and scaling the resuling application.

An easy, automated deployment is possible in both cases by automatically creating the model, which can then be stored and accesses via an existing application.

Also, the factor of scalability is met in both cases as well. Azure ML services run on a scalable cluster on the Azure Cloud. This cluster scales automatically depending on the necessary resources for each step. Kubeflow, on the other hand, runs directly on any Kubernetes cluster. This enables scalability, as well. The resources can even be specified by the users if need be.

Last, **operation** of the resulting model is of a high importance for fulfilling the need of DevOps. 

While neither of both offer opportunities to monitor the usage of the model directly on the pipeline, both provide the possibility of *continuously refitting the model*. Because of the automated steps these pipelines are running through, the developer is released of the burden to repeat running all those tasks manually when the model should be refitted. The pipelines can be run sequentially, so that as soon as the data has been updated, the model can be retrained.

In summary, these two frameworks are built for different use-cases, and both serve their purposes well. Azure ML service is for a quick building of a pipeline, which only supports the most basic common use cases and is dependent on what Microsoft provides. These are the typical disadvantages for such a standardized SaaS solution. So it is a good way to choose if some uncomplicated model should be built, which does not need a lot of special configurations. 

Kubeflow, on the other hand, offers the developer a free choice of tools and frameworks being used, so that there is no loss in functionality. Additionally, it offers the user to set a period of time, in which the pipeline will be triggered automatically. This enables a good way to continuously improve the model by continuously updating the dataset and automatically rebuild the model sequentially. 

It can not only setup locally as described in chapter \ref{sec:kubeflow} but can be built on top of every Kubernetes cluster, no matter if is a local cluster or provided on a Cloud.

In return, the building effort is even a bit higher compared to local development. This is why using Kubeflow pays off if some components can be reused, and not everything has to be built from scratch over and over again. If a long term project needs similar components for several models, which should be continuously improved, the effort to build this pipeline may be worth it.

## Facing the challenges adapting Development Operations for AI {#sec:devopsaieval}

Development Operations for Artificial Intelligence is still a new topic in the world of IT and still evolving a lot. During the time of this project the termology of this topic has shifted from AIOps to MLOps and many new articles has been released. This shows how this new topic is still changing a lot and there is no final solution yet. The success of adapting traditional DevOps practices and methodologies can be discussed by comparing it to the four original principles: Develop and test against production-like systems, deploy with repeatable and reliable processes, monitor and validate operational quality and amplify feedback loops, on which the whole DevOps movement is based on.

First, for developing and testing against production-like systems, different factors play a role. On the one hand, one objective is to properly built and test the models inside an environment which behaves similar to the production environment. On the other hand, also scalability and flexibility is from importance for Operations for Machine Learning, because some steps needs more resources than others and depending on the usage of a finished model the necessary resources can vary a lot.

That's why the defined practice of using containers for the single components during the development, testing and deployment has been defined. This creates one environment, that does not differ in the development and production environment. When deploying this on a scalable infrastructure, as proposed in chapter \ref{sec:devopsai}, scalability is also achieved. For this the described Cloud technologies (see chapter \ref{sec:ms12}) can be extremly useful. As mentioned above, Cloud technologies are distinguishing themselves by their flexibility and scalability, which is why Cloud offers infrastructures as a service, which are fulfilling all these needs, for example Kubernetes.

Next, deploying with repeatable and reliable processes is an important principle in Development Operations. So is it also for Machine Learning operations. Because of the many, different steps to be executed during the development of an AI model, the whole development cycle should be automated as far as possible, so that these steps does not have to be done manually over and over again. A pipeline as presented in chapter \ref{sec:result} faces this requirement through automating the whole development cycle and supports it until deployment of the model. Here Cloud technologies can help as well by offering a SaaS, which enables building such a pipeline as mentioned in chapter \ref{sec:azurepip}. With such pipelines the developer does only have to take care about handling and cleaning the data as well as choosing good parameters and a good architecture for the model. If a single step has to be changed during the model preparations or training this can be done by simply adjusting the pertaining component. Through this the whole process of development and deployment is repeatable and reliable.

Also the monitoring and validating of operational quality can be supported by such a pipeline, because the testing of the model should be a part of it. The practice of integrating such tests and continuously executing them is already existent in traditional DevOps. This is the same for Machine Learning operations, but there this practice is extended by testing the model with validation data to guarantee an acceptable accuracy of the model. Only through repeatably executing such tests before every deployment this principle can be met.

Last, amplifying feedback loops has an even higher importance for Machine Learning operations, because *continuous refitting of the model* can be done automatically by retraining the model with new data. This requires continuous data collection and a versionized data storage, so that in case of issues the data and the model can be reset to a time, where everything was still working. Direct user feedback can be used as valuable data for training the model and increasing its performance. With the practice of automatically collecting and storing the data these feedback loops can be integrated into the process of ML development and deployment.

All in all, this shows, that the principles of DevOps can be applied for Machine Learning operations. Some practices needed to be adapted or extended and some other had to be created from scratch. The greatest lack of DevOps for Machine Learning are non-existing technologies to implement all those practices into the development cycle. As discussed in chapter \ref{sec:pipelineeval} tools for building pipelines are existing, but still there is need for improvement. Also data collaboration platforms as well as IDE integration is existing but not perfectly integrated into the development cycle. This will be one important task for the next years, to increase the efficiency of this new kind of software development. Those requirements for the future will be discussed in chapter \ref{sec:outlook}.

## Outlook {#sec:outlook}

In the future, Machine Learning will be more and more important, and more and more complex systems will be built. This requires automated development and delivery processes, which is the reason why DevOps for Machine Learning will arise even more over time. 

ML projects in the future need to be more scalable because the demand and the complexity of AI will rise, which needs more resources. At the same time, the development of such models will be getting more complex because of the rising possibilities of AI. This needs practices to keep up with collaborative, organized, and flexible development as there is already for common software development.

Kubeflow or Azure Machine Learning service can be a part of the solution in the future to automate the development and delivery processes. Both do not offer a complete solution because things like a collaborative data platform are supported but note directly integrated. Additionally, both systems have their problems to deal with. In case of Azure Machine Learning service, this is missing functionality and flexibility, in case of Kubeflow flexibility and functionality is given but in return, the development of such a pipeline is still comparatively complex, and the system is not easy to handle. 

There is no final solution for out-of-the-box DevOps for Machine Learning yet. There are still a lot of things to do, like combining good data and model versioning with an easy way to automate the development and deployment. Kubeflow is on a good way to handle most of the things, but the system is still pretty complex. But Kubeflow is still in development and far from finished, so there is a good chance, that it will evolve as a good possibility to support big Machine Learning projects in the future.

All this shows that promising approaches and technologies are on its way. They will still evolve over time, but in the future they can help Data Scientists to build and operate AI models at the same speed and agility as it is already done in traditional software development. This way, in the future an integration of traditional Software and ML development and operation can be achieved, which will increase the efficiency and quality of products a lot.